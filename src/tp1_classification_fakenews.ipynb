{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tache de classification sur des articles de journal. \n",
    "\n",
    "The Fake News Classification Dataset is an English-language dataset containing just over 45,000 unique news articles. These articles are classified as true (1) or false (0)\n",
    "\n",
    "link : https://www.kaggle.com/datasets/aadyasingh55/fake-news-classification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pandas import DataFrame\n",
    "\n",
    "import os,re\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import tqdm\n",
    "import importlib\n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "#Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Kagglehub\n",
    "import kagglehub\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "#NLP modules\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    trainers)\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Deeplearning with Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # CPU or GPU \n",
    "print(cores,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/tp1\",exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"aadyasingh55/fake-news-classification\",force_download=True)\n",
    "\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    print(dirname)\n",
    "    print(_)\n",
    "    print(filenames)\n",
    "    for filename in filenames:\n",
    "        Path(os.path.join(dirname, filename)).rename(os.path.join(\"../data/tp1\", filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!file -i \"../data/tp1/train (2).csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_train = os.path.join(\"../data/tp1\", \"train (2).csv\")\n",
    "path_test = os.path.join(\"../data/tp1\", \"test (1).csv\")\n",
    "path_eval= os.path.join(\"../data/tp1\",\"evaluation.csv\" )\n",
    "\n",
    "encoding='utf-8'\n",
    "\n",
    "# Load training data\n",
    "try:\n",
    "    train_df = pd.read_csv(path_train, on_bad_lines='skip',sep=';',index_col=0,encoding=encoding)\n",
    "    print(\"Train dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading train dataset: {e}\")\n",
    "\n",
    "# Load evaluation data\n",
    "try:\n",
    "    eval_df = pd.read_csv(path_eval, on_bad_lines='skip',sep=';',index_col=0,encoding=encoding)\n",
    "    print(\"Evaluation dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading evaluation dataset: {e}\")\n",
    "\n",
    "# Load test data\n",
    "try:\n",
    "    test_df = pd.read_csv(path_test, on_bad_lines='skip',sep=';',index_col=0,encoding=encoding)\n",
    "    print(\"Test dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.info())\n",
    "print(eval_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des données \n",
    "\n",
    "### Question 1 :\n",
    "- Compléter la fonction \"display_random_sample\" pour permettre d'afficher de manière aléatoire des exemples du jeu de données.\n",
    "- Créer une colonne (\"content\") qui est la concaténation du titre et de l'article.\n",
    "- visualiser la répartition des classes entre les faux et les vrais articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_sample(dataset:DataFrame, num_examples:int=1)->None:\n",
    "    pass\n",
    "\n",
    "display_random_sample(train_df,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization des données, entrainer un tokenizer et visualisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 :\n",
    "À l'aide des deux liens ci-dessous, entraîner un tokenizer BPE sur l'ensemble de votre jeu de données.\n",
    "\n",
    "\n",
    "Build a tokenizer from scratch = https://huggingface.co/docs/tokenizers/quicktour\n",
    "\n",
    "Building a tokenizer, block by block = https://huggingface.co/learn/nlp-course/chapter6/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate training corpus from dataset\n",
    "def get_training_corpus(dataset, column_name=\"content\"):\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[column_name].iloc[i : i + 1000].astype(str).tolist()\n",
    "\n",
    "# Initialize the tokenizer with a BPE model\n",
    "tokenizer = None\n",
    "\n",
    "# Set normalizers (optional: lowercase, NFD, etc.)\n",
    "tokenizer.normalizer = normalizers.Sequence(None)\n",
    "\n",
    "# Set pre-tokenizer\n",
    "tokenizer.pre_tokenizer = None\n",
    "\n",
    "# Define the BPE trainer with a vacab_size = 25_000\n",
    "#[...]\n",
    "\n",
    "# Train the tokenizer\n",
    "#[...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/tokenizers\",exist_ok=True)\n",
    "#Save the tokenizer\n",
    "tokenizer.save(\"../data/tokenizers/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"../data/tokenizers/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : \n",
    "Expliquer les sorties de \"tokenizer.encode(txt)\", ajouter une nouvelle colonne aux datasets avec la liste des \"ids\" pour chaque \"content\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 1 original content\n",
    "sample_text = train_df.sample().content.values[0]\n",
    "\n",
    "# Encode the sample text using the tokenizer\n",
    "#[...]\n",
    "\n",
    "# Print results in a structured way\n",
    "print(\"Original Text:\")\n",
    "#[...]\n",
    "\n",
    "print(\"\\nTokenized Output (Tokens):\")\n",
    "#[...]\n",
    "\n",
    "print(\"\\nTokenized Output (IDs):\")\n",
    "#[...]\n",
    "\n",
    "print(\"\\nNumber of Tokens:\")\n",
    "#[...]\n",
    "\n",
    "print(\"\\nDecoded Text:\")\n",
    "#[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 4 :\n",
    "Proposer une visualisation sous forme de nuage de mots pour les faux  et les vrais articles. Vous pourrez exclure les mots de liaisons et la ponctuation (\"a\", \"the\", \",\" ...) qui n'apportent pas d'information en utilisant la bibliothèque \"nlk\" et sa base de \"stop_word\" en anglais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat_df = pd.concat([train_df,eval_df],axis=0)\n",
    "concat_df[\"tokens\"] = concat_df[\"content\"].apply(lambda txt : tokenizer.encode(txt).tokens)\n",
    "concat_df[\"ids\"]= concat_df[\"content\"].apply(lambda txt : tokenizer.encode(txt).ids) \n",
    "\n",
    "# Step 1: Extract Vocabulary\n",
    "vocab = tokenizer.get_vocab()  # Returns a dictionary {word: token_id}\n",
    "id_to_word = {id: word for word, id in vocab.items()}  # Reverse mapping\n",
    "\n",
    "\n",
    "# Step 2: Count Word Frequencies\n",
    "# Flatten the list of token IDs\n",
    "all_ids  = [id for ids in concat_df[\"ids\"] for id in ids]\n",
    "all_ids_fake = [id for ids in concat_df[concat_df[\"label\"] == 0][\"ids\"] for id in ids]\n",
    "all_ids_true = None #[...]\n",
    "\n",
    "# Count the frequency of each token ID\n",
    "id_counts = Counter(all_ids)\n",
    "id_counts_fake = None\n",
    "id_counts_true = None\n",
    "\n",
    "# Step 3: Map Token IDs to Words\n",
    "# Convert token frequencies to word frequencies\n",
    "word_frequencies = {id_to_word[id]: count for id, count in id_counts.items() if id in id_to_word}\n",
    "word_frequencies_fake = None\n",
    "word_frequencies_true = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "ponctuation_set = {'(', ')', ',', '-', '.', ':', '’', '“'}\n",
    "# Get the English stop word list and add your ponctuation stop word set\n",
    "stop_words = set(stopwords.words('english')).union(ponctuation_set)\n",
    "\n",
    "# Stop word filtration \n",
    "# #[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Word Cloud\n",
    "#link : https://www.kaggle.com/code/niteshhalai/wordcloud-colormap\n",
    "\n",
    "# Create the WordCloud object\n",
    "#[...]\n",
    "#[...]\n",
    "#[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 :  \n",
    "En utilisant la bibliothèque Gesim entrainer un model Word2Vec avec votre jeu de données et votre tokenizer. Explorer ensuite les représentations vectorielles renvoyées par le model sur quelques exemples (\"trump\", \"said\", \"president\", \"people\", ...)\n",
    "\n",
    "link1 : https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673\n",
    "\n",
    "link2 : https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "link3 : https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyIter:\n",
    "\n",
    "    def __init__(self,sentences,tokenizer):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            yield list(tokenizer.encode(sentence).tokens)\n",
    "\n",
    "\n",
    "concat_df = pd.concat([train_df,eval_df],axis=0)\n",
    "\n",
    "sentences_iterator = MyIter(sentences = concat_df[\"content\"].to_list(),tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = None\n",
    "\n",
    "w2v_model.build_vocab(sentences_iterator)  # prepare the model vocabulary\n",
    "\n",
    "for epoch in tqdm.tqdm(range(5)):\n",
    "    #[...]\n",
    "    pass\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../models_checkpoints/tp1\",exist_ok=True)\n",
    "#save the model Word2Vec \n",
    "w2v_model.save(\"../models_checkpoints/tp1/fakenews_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"../models_checkpoints/tp1/fakenews_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration des embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"president\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"said\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainer un modèle de classification avec des poids aléatoires "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : \n",
    "\n",
    "* Construire des datasets train, eval et test avec la classe Dataset. \n",
    "\n",
    "link1 : https://plainenglish.io/blog/understanding-collate-fn-in-pytorch-f9d1742647d3\n",
    "\n",
    "link2 : http://www.idris.fr/jean-zay/gpu/jean-zay-gpu-torch-data-preprocessing.html\n",
    "\n",
    "* Ajout d'une colonne \"input_ids\" avec la méthode map\n",
    "\n",
    "link : https://medium.com/@sujathamudadla1213/what-are-some-benefits-of-using-the-dataset-map-7dab03afb6d3\n",
    "\n",
    "* Ajouter un index de padding pour les positions a ignorer par le modèle où encore pour compléter dans chaque batch pour obtenir une matrice\n",
    "\n",
    "* Construire des dataloaders associés qui rajoutent le padding pour chaque batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforme a pandas df to a torch dataset object \n",
    "#[...]\n",
    "#[...]\n",
    "#[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function for tokenization\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer.encode(example[\"content\"]).ids\n",
    "    return {\"input_ids\": tokens}\n",
    "#[...]\n",
    "#[...]\n",
    "#[...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'un index de padding pour les positions a ignorer par le modèle où encore pour compléter dans chaque batch pour obtenir une matrice\n",
    "pad_idx = None\n",
    "\n",
    "print(\"L'index de padding est :\", pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for padding.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    " #[...]\n",
    " # #[...]\n",
    " # #[...]\n",
    " # #[...]   \n",
    ")\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    #[...]\n",
    "    #[...]\n",
    "    #[...]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"input_ids\"].shape)  # Shape of the input_ids tensor\n",
    "    print(batch[\"labels\"])\n",
    "    print(batch[\"labels\"].shape)  # Shape of the labels tensor\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 :\n",
    "\n",
    "Developper une architecture de modèle héritant de la classe \"nn.Module\" qui converti les \"input_ids\" en embeddings, les moyennes pour donner une représentation vectorielle à chaque article. Le modèle doit renvoyer une prédiction entre deux classes :\n",
    "\n",
    "link1 : https://medium.com/@spandey8312/text-classification-using-custom-data-and-pytorch-d88ba1087045\n",
    "\n",
    "Remarque : vous pourrez utiliser le model Word2Vec précédemment entrainé pour initialiser la table d'embeddings du classifier  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifierMean(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_model, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifierNormL2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_model, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialisation des modèles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 100\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier_1 = TextClassifierMean(vocab_size, embedding_dim,w2v_model, num_classes)\n",
    "\n",
    "# exemple aléatoire de \"inputs_ids\"\n",
    "inputs_ids_test = torch.randint(low=0,high=200,size=(3,14)) #batch_size, Idx_token \n",
    "print(inputs_ids_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_classifier_1.eval() #permet de ne pas prendre en compte le dropout\n",
    "    output_model_test = model_classifier_1(inputs_ids_test)\n",
    "    \n",
    "    print(output_model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_classifier_2 = TextClassifierNormL2(vocab_size, embedding_dim,w2v_model, num_classes)\n",
    "\n",
    "# exemple aléatoire de \"inputs_ids\"\n",
    "inputs_ids_test = torch.randint(low=0,high=200,size=(3,14)) \n",
    "print(inputs_ids_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_classifier_2.eval() #permet de ne pas prendre en compte le dropout\n",
    "    output_model_test = model_classifier_2(inputs_ids_test)\n",
    "    \n",
    "    print(output_model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérifier que la table d'embeddings du model est bien synchronisée avec les embeddings du model Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_items = list(tokenizer.get_vocab().items())\n",
    "random_token, random_index = vocab_items[np.random.choice(len(vocab_items))]\n",
    "\n",
    "print(\"Randomly selected token:\", random_token)\n",
    "print(\"Index of the token:\", random_index)\n",
    "\n",
    "try:\n",
    "    word_vector = torch.tensor(w2v_model.wv[random_token])\n",
    "    print( word_vector == model_classifier_1.embedding.weight.data[random_index])\n",
    "\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : \n",
    "\n",
    "Reprendre la fonction \"train_test\" dans \"module_train_tp1.py\" et entrainer les poids de votre modèle sur 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import module_train_tp1 as trainer\n",
    "importlib.reload(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_classifier_1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../models_checkpoints/tp1\",exist_ok=True)\n",
    "os.makedirs(\"./logs/tp1\",exist_ok=True)\n",
    "\n",
    "time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "model_class = \"TextClassifierMean_\"\n",
    "\n",
    "model_name = model_class + time + \".checkpoint.pth\"\n",
    "checkpoint_path = os.path.join(\"../models_checkpoints/tp1\",model_name)\n",
    "\n",
    "log_dir = \"logs/tp1/\" + model_class + time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_train_loss, list_eval_loss ,list_train_accuracy, list_eval_accuracy ,list_train_f1, list_eval_f1 = trainer.train_test(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_metrics(list_train_loss, list_eval_loss, \"cross entropie loss\")\n",
    "trainer.plot_metrics(list_train_accuracy, list_eval_accuracy , \"accuracy\")\n",
    "trainer.plot_metrics(list_train_f1, list_eval_f1, \"f1_score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 :\n",
    "Evaluer le model sur les données test et exhiber des exemples d'articles prédit avec un mauvais label. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement des modèles entrainés\n",
    "\n",
    "#model_name = \"TextClassifierMean_20250130_1631.checkpoint.pth\"\n",
    "checkpoint_path = os.path.join(\"../models_checkpoints/tp1\",model_name)\n",
    "\n",
    "w2v_model = Word2Vec.load(\"../models_checkpoints/tp1/fakenews_word2vec.model\")\n",
    "\n",
    "\n",
    "model= TextClassifierMean(vocab_size, embedding_dim,w2v_model, num_classes)\n",
    "model.load_state_dict(torch.load(checkpoint_path,weights_only=False))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation du model sur le jeu de données test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle de classification sur les données test\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "total_test_samples = 0\n",
    "\n",
    "correct_predictions_test = 0\n",
    "\n",
    "predict_labels_test_list = []\n",
    "targets_test_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        pass\n",
    "\n",
    "loss_test = None\n",
    "\n",
    "\n",
    "accuracy_test = None\n",
    "precision_test = None\n",
    "recall_test = None\n",
    "f1_score_test =  None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the metrics in a clear format\n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"Loss: {loss_test:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des mauvaises predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prediction_for_row(row,tokenizer,model):\n",
    "    predicted_label = None\n",
    "    return predicted_label\n",
    "\n",
    "# Partial function to pass fixed arguments\n",
    "\n",
    "\n",
    "\n",
    "# Map the prediction function to each row\n",
    "\n",
    "\n",
    "# Optionally convert back to a pandas DataFrame if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_prediction_df = None\n",
    "good_prediction_df = None\n",
    "\n",
    "display_random_sample(error_prediction_df[['title', 'text', 'label','predict_label']],1) \n",
    "display_random_sample(good_prediction_df[['title', 'text', 'label','predict_label']],1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "##link : https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "\n",
    "# Generate the confusion matrix\n",
    "y_true = None\n",
    "y_pred = None\n",
    "confu_mat = None\n",
    "\n",
    "# Define group names\n",
    "group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "\n",
    "# Compute counts for each group\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in confu_mat.flatten()]\n",
    "\n",
    "# Compute percentages for each group\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in confu_mat.flatten() / np.sum(confu_mat)]\n",
    "\n",
    "# Combine names, counts, and percentages into a single label\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "\n",
    "# Reshape labels to match the confusion matrix shape\n",
    "labels = np.asarray(labels).reshape(confu_mat.shape)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confu_mat, annot=labels, fmt='', cmap='Blues')\n",
    "plt.title('Confusion Matrix with Annotations')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposer votre des exemples d'articles aliant fake-news et vrais articles et observez comment votre modèle généralise son apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link = https://library-nd.libguides.com/fakenews/examples\n",
    "\n",
    "content_fake = \"\"\"\n",
    "Hey Facebook, As some of you may know, I'm Bill Gates. If you click that share link, I will give you $5,000. \n",
    "I always deliver, I mean, I brought you Windows XP, right?\n",
    "\"\"\"\n",
    "\n",
    "row = {\n",
    "    \"content\" : content_fake}\n",
    "\n",
    "label = None\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_fake = \"\"\"\n",
    "On 15 May 2014, the National Report published an article reporting that Barack Obama had (in defiance of constitutional provisions) shockingly proclaimed he would be seeking a third term as President of the United States:\n",
    "\n",
    "President Barack Obama shocked the country this morning with news that he is running for a third term\n",
    "\n",
    "\"I can't abandon the American people now when they need me more than ever,\" Obama told reporters at a press conference this morning. \"We've come this far as a nation, now is not the time to do something different. This is the change you wanted and this is the change you’re getting.\" \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "row = {\n",
    "    \"content\" : content_fake}\n",
    "\n",
    "label = None\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link = https://www.bbc.com/news/articles/cq5eewvy3nlo\n",
    "\n",
    "content_true= \"\"\"\n",
    "Donald Trump was rushed to safety on Sunday after what the FBI termed an apparent assassination attempt at his golf course in West Palm Beach, Florida.\n",
    "\n",
    "Ryan Wesley Routh, 58, was arrested and charged with firearms offences.\n",
    "\n",
    "The incident comes almost exactly two months after a shooting at a Trump rally in Butler, Pennsylvania, which left the Republican nominee with minor injuries and killed a man in the crowd.\n",
    "\n",
    "Here is what we know so far about Sunday.\n",
    "How was the suspect spotted?\n",
    "\n",
    "The incident unfolded at the Trump International Golf Club in West Palm Beach, about 15 minutes from Trump's Florida residence, Mar-a-Lago.\n",
    "\n",
    "A gunman was first seen by Secret Service agents, who were sweeping the course. Agents usually go one hole ahead to perform security checks, according to police.\n",
    "\n",
    "The muzzle of a rifle - an SKS semi-automatic - was spotted sticking through the shrubbery that lines the course.\n",
    "\n",
    "At the time, Trump was about 300-500 yards (275-460m) away from the spot.\n",
    "\n",
    "An agent \"immediately engaged\" with the person holding the rifle, who fled, Sheriff Rik Bradshaw said. The suspect did not fire his weapon during the incident.\n",
    "\n",
    "On 16 September, acting Secret Service director Ron Rowe said Trump was \"across the course and out of sight of the sixth green\" when the Secret Service agent opened fire.\n",
    "\n",
    "At no point is the suspect believed to have had a clear line of sight to the former president.\n",
    "\n",
    "Records show that Routh's phone had been in the area for about 12 hours, from around 02:00 to 13:31 local time, according to court documents and police officials. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "row = {\n",
    "    \"content\" : content_true}\n",
    "\n",
    "label = None\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link = https://www.bbc.com/news/articles/cr56gr6p49eo\n",
    "\n",
    "content_true=\"\"\"Russia claims that its forces have captured the front-line town of Kurakhove in eastern Ukraine's Donetsk region.\n",
    "\n",
    "The town has borne the brunt of Russian advances in recent months and is a stepping-stone to the key logistical hub of Pokrovsk.\n",
    "\n",
    "Ukraine has not acknowledged the fall of Kurakhove, which is 35km (21 miles) south of Pokrovsk.\n",
    "\n",
    "Fierce fighting has also been under way in Russia's Kursk region in recent days after Ukraine launched a counter-attack on Sunday.\n",
    "\n",
    "An image taken from social media and supplied by the Reuters news agency appears to show a soldier holding up a Russian flag in Kurakhove. The image has not been verified by the BBC.\n",
    "\n",
    "Viktor Trehubov, spokesperson for Ukraine's Khortytsia group of forces, told Reuters news agency that, as of Monday morning, Ukrainian forces were still engaging Russian troops inside Kurakhove.\n",
    "\n",
    "Kurakhove is linked to Pokrovsk by roads that are part of the infrastructure to move troops and supplies along the front line.\n",
    "\n",
    "The taking of Kurakhove would allow the Russians to go north to attack Pokrovsk from a new direction, analyst Roman Pohorily said.\n",
    "\n",
    "Russia's defence ministry also claimed on Monday that the village of Dachenske, which about 8km south of Pokrovsk, had been captured by its forces.\n",
    "\n",
    "Kyiv's forces are reportedly suffering from manpower shortages and have been losing ground in the east of Ukraine in recent months, as Russian troops advance.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "row = {\n",
    "    \"content\" : content_true}\n",
    "\n",
    "label = None\n",
    "\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
